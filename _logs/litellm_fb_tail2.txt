ai_gateway-litellm-fb-1  |     )
ai_gateway-litellm-fb-1  | litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=65.0, time taken=65.5 seconds
ai_gateway-litellm-fb-1  | 
ai_gateway-litellm-fb-1  | During handling of the above exception, another exception occurred:
ai_gateway-litellm-fb-1  | 
ai_gateway-litellm-fb-1  | Traceback (most recent call last):
ai_gateway-litellm-fb-1  |   File "/usr/lib/python3.13/site-packages/litellm/proxy/proxy_server.py", line 4092, in chat_completion
ai_gateway-litellm-fb-1  |     result = await base_llm_response_processor.base_process_llm_request(
ai_gateway-litellm-fb-1  |              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ai_gateway-litellm-fb-1  |     ...<16 lines>...
ai_gateway-litellm-fb-1  |     )
ai_gateway-litellm-fb-1  |     ^
ai_gateway-litellm-fb-1  |   File "/usr/lib/python3.13/site-packages/litellm/proxy/common_request_processing.py", line 438, in base_process_llm_request
ai_gateway-litellm-fb-1  |     responses = await llm_responses
ai_gateway-litellm-fb-1  |                 ^^^^^^^^^^^^^^^^^^^
ai_gateway-litellm-fb-1  |   File "/usr/lib/python3.13/site-packages/litellm/router.py", line 1076, in acompletion
ai_gateway-litellm-fb-1  |     raise e
ai_gateway-litellm-fb-1  |   File "/usr/lib/python3.13/site-packages/litellm/router.py", line 1052, in acompletion
ai_gateway-litellm-fb-1  |     response = await self.async_function_with_fallbacks(**kwargs)
ai_gateway-litellm-fb-1  |                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ai_gateway-litellm-fb-1  |   File "/usr/lib/python3.13/site-packages/litellm/router.py", line 3895, in async_function_with_fallbacks
ai_gateway-litellm-fb-1  |     return await self.async_function_with_fallbacks_common_utils(
ai_gateway-litellm-fb-1  |            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ai_gateway-litellm-fb-1  |     ...<8 lines>...
ai_gateway-litellm-fb-1  |     )
ai_gateway-litellm-fb-1  |     ^
ai_gateway-litellm-fb-1  |   File "/usr/lib/python3.13/site-packages/litellm/router.py", line 3853, in async_function_with_fallbacks_common_utils
ai_gateway-litellm-fb-1  |     raise original_exception
ai_gateway-litellm-fb-1  |   File "/usr/lib/python3.13/site-packages/litellm/router.py", line 3887, in async_function_with_fallbacks
ai_gateway-litellm-fb-1  |     response = await self.async_function_with_retries(*args, **kwargs)
ai_gateway-litellm-fb-1  |                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ai_gateway-litellm-fb-1  |   File "/usr/lib/python3.13/site-packages/litellm/router.py", line 4092, in async_function_with_retries
ai_gateway-litellm-fb-1  |     raise original_exception
ai_gateway-litellm-fb-1  |   File "/usr/lib/python3.13/site-packages/litellm/router.py", line 3983, in async_function_with_retries
ai_gateway-litellm-fb-1  |     response = await self.make_call(original_function, *args, **kwargs)
ai_gateway-litellm-fb-1  |                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ai_gateway-litellm-fb-1  |   File "/usr/lib/python3.13/site-packages/litellm/router.py", line 4101, in make_call
ai_gateway-litellm-fb-1  |     response = await response
ai_gateway-litellm-fb-1  |                ^^^^^^^^^^^^^^
ai_gateway-litellm-fb-1  |   File "/usr/lib/python3.13/site-packages/litellm/router.py", line 1357, in _acompletion
ai_gateway-litellm-fb-1  |     raise e
ai_gateway-litellm-fb-1  |   File "/usr/lib/python3.13/site-packages/litellm/router.py", line 1309, in _acompletion
ai_gateway-litellm-fb-1  |     response = await _response
ai_gateway-litellm-fb-1  |                ^^^^^^^^^^^^^^^
ai_gateway-litellm-fb-1  |   File "/usr/lib/python3.13/site-packages/litellm/utils.py", line 1598, in wrapper_async
ai_gateway-litellm-fb-1  |     raise e
ai_gateway-litellm-fb-1  |   File "/usr/lib/python3.13/site-packages/litellm/utils.py", line 1449, in wrapper_async
ai_gateway-litellm-fb-1  |     result = await original_function(*args, **kwargs)
ai_gateway-litellm-fb-1  |              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ai_gateway-litellm-fb-1  |   File "/usr/lib/python3.13/site-packages/litellm/main.py", line 565, in acompletion
ai_gateway-litellm-fb-1  |     raise exception_type(
ai_gateway-litellm-fb-1  |           ~~~~~~~~~~~~~~^
ai_gateway-litellm-fb-1  |         model=model,
ai_gateway-litellm-fb-1  |         ^^^^^^^^^^^^
ai_gateway-litellm-fb-1  |     ...<3 lines>...
ai_gateway-litellm-fb-1  |         extra_kwargs=kwargs,
ai_gateway-litellm-fb-1  |         ^^^^^^^^^^^^^^^^^^^^
ai_gateway-litellm-fb-1  |     )
ai_gateway-litellm-fb-1  |     ^
ai_gateway-litellm-fb-1  |   File "/usr/lib/python3.13/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2301, in exception_type
ai_gateway-litellm-fb-1  |     raise e
ai_gateway-litellm-fb-1  |   File "/usr/lib/python3.13/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2270, in exception_type
ai_gateway-litellm-fb-1  |     raise APIConnectionError(
ai_gateway-litellm-fb-1  |     ...<4 lines>...
ai_gateway-litellm-fb-1  |     )
ai_gateway-litellm-fb-1  | litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=65.0, time taken=65.5 seconds. Received Model Group=code.hybrid
ai_gateway-litellm-fb-1  | Available Model Group Fallbacks=None LiteLLM Retried: 1 times, LiteLLM Max Retries: 2
ai_gateway-litellm-fb-1  | [92m23:59:12 - LiteLLM:WARNING[0m: utils.py:528 - `litellm.set_verbose` is deprecated. Please set `os.environ['LITELLM_LOG'] = 'DEBUG'` for debug logs.
ai_gateway-litellm-fb-1  | Initialized litellm callbacks, Async Success Callbacks: [<bound method Router.deployment_callback_on_success of <litellm.router.Router object at 0x702e019101a0>>, <litellm.proxy.hooks.model_max_budget_limiter._PROXY_VirtualKeyModelMaxBudgetLimiter object at 0x702e029482f0>, <litellm.proxy.hooks.max_budget_limiter._PROXY_MaxBudgetLimiter object at 0x702e018f1d10>, <litellm.proxy.hooks.parallel_request_limiter._PROXY_MaxParallelRequestsHandler object at 0x702e018f2c10>, <litellm.proxy.hooks.cache_control_check._PROXY_CacheControlCheck object at 0x702e018f2ad0>, <litellm_enterprise.proxy.hooks.managed_files._PROXY_LiteLLMManagedFiles object at 0x702e01910ec0>, <litellm._service_logger.ServiceLogging object at 0x702e02bb8b00>]
ai_gateway-litellm-fb-1  | Inside Max Parallel Request Pre-Call Hook
ai_gateway-litellm-fb-1  | ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None
ai_gateway-litellm-fb-1  | Final returned optional params: {'temperature': 0.2, 'stream': False, 'num_predict': 64}
ai_gateway-litellm-fb-1  | Async Wrapper: Completed Call, calling async_success_handler: <bound method Logging.async_success_handler of <litellm.litellm_core_utils.litellm_logging.Logging object at 0x702dff84bb10>>
ai_gateway-litellm-fb-1  | Logging Details LiteLLM-Async Success Call, cache_hit=None
ai_gateway-litellm-fb-1  | Async success callbacks: Got a complete streaming response
ai_gateway-litellm-fb-1  | INSIDE parallel request limiter ASYNC SUCCESS LOGGING
ai_gateway-litellm-fb-1  | updated_value in success call: {'current_requests': 0, 'current_tpm': 74, 'current_rpm': 1}, precise_minute: 2025-09-11-23-59
ai_gateway-litellm-fb-1  | updated_value in success call: {'current_requests': 0, 'current_tpm': 74, 'current_rpm': 0}, precise_minute: 2025-09-11-23-59
ai_gateway-litellm-fb-1  | updated_value in success call: {'current_requests': 0, 'current_tpm': 74, 'current_rpm': 1}, precise_minute: 2025-09-11-23-59
ai_gateway-litellm-fb-1  | INFO:     172.22.0.1:44442 - "POST /v1/chat/completions HTTP/1.1" 200 OK
ai_gateway-litellm-fb-1  | [92m23:59:20 - LiteLLM:WARNING[0m: utils.py:528 - `litellm.set_verbose` is deprecated. Please set `os.environ['LITELLM_LOG'] = 'DEBUG'` for debug logs.
ai_gateway-litellm-fb-1  | Initialized litellm callbacks, Async Success Callbacks: [<bound method Router.deployment_callback_on_success of <litellm.router.Router object at 0x702e019101a0>>, <litellm.proxy.hooks.model_max_budget_limiter._PROXY_VirtualKeyModelMaxBudgetLimiter object at 0x702e029482f0>, <litellm.proxy.hooks.max_budget_limiter._PROXY_MaxBudgetLimiter object at 0x702e018f1d10>, <litellm.proxy.hooks.parallel_request_limiter._PROXY_MaxParallelRequestsHandler object at 0x702e018f2c10>, <litellm.proxy.hooks.cache_control_check._PROXY_CacheControlCheck object at 0x702e018f2ad0>, <litellm_enterprise.proxy.hooks.managed_files._PROXY_LiteLLMManagedFiles object at 0x702e01910ec0>, <litellm._service_logger.ServiceLogging object at 0x702e02bb8b00>]
ai_gateway-litellm-fb-1  | Inside Max Parallel Request Pre-Call Hook
ai_gateway-litellm-fb-1  | ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None
ai_gateway-litellm-fb-1  | Final returned optional params: {'temperature': 0.2, 'stream': False, 'num_predict': 64}
ai_gateway-litellm-fb-1  | Async Wrapper: Completed Call, calling async_success_handler: <bound method Logging.async_success_handler of <litellm.litellm_core_utils.litellm_logging.Logging object at 0x702dff84bd90>>
ai_gateway-litellm-fb-1  | Logging Details LiteLLM-Async Success Call, cache_hit=None
ai_gateway-litellm-fb-1  | Async success callbacks: Got a complete streaming response
ai_gateway-litellm-fb-1  | INSIDE parallel request limiter ASYNC SUCCESS LOGGING
ai_gateway-litellm-fb-1  | updated_value in success call: {'current_requests': 0, 'current_tpm': 150, 'current_rpm': 2}, precise_minute: 2025-09-11-23-59
ai_gateway-litellm-fb-1  | updated_value in success call: {'current_requests': 0, 'current_tpm': 150, 'current_rpm': 0}, precise_minute: 2025-09-11-23-59
ai_gateway-litellm-fb-1  | updated_value in success call: {'current_requests': 0, 'current_tpm': 150, 'current_rpm': 2}, precise_minute: 2025-09-11-23-59
ai_gateway-litellm-fb-1  | INFO:     172.22.0.1:57968 - "POST /v1/chat/completions HTTP/1.1" 200 OK
ai_gateway-litellm-fb-1  | [92m23:59:26 - LiteLLM:WARNING[0m: utils.py:528 - `litellm.set_verbose` is deprecated. Please set `os.environ['LITELLM_LOG'] = 'DEBUG'` for debug logs.
ai_gateway-litellm-fb-1  | Initialized litellm callbacks, Async Success Callbacks: [<bound method Router.deployment_callback_on_success of <litellm.router.Router object at 0x702e019101a0>>, <litellm.proxy.hooks.model_max_budget_limiter._PROXY_VirtualKeyModelMaxBudgetLimiter object at 0x702e029482f0>, <litellm.proxy.hooks.max_budget_limiter._PROXY_MaxBudgetLimiter object at 0x702e018f1d10>, <litellm.proxy.hooks.parallel_request_limiter._PROXY_MaxParallelRequestsHandler object at 0x702e018f2c10>, <litellm.proxy.hooks.cache_control_check._PROXY_CacheControlCheck object at 0x702e018f2ad0>, <litellm_enterprise.proxy.hooks.managed_files._PROXY_LiteLLMManagedFiles object at 0x702e01910ec0>, <litellm._service_logger.ServiceLogging object at 0x702e02bb8b00>]
ai_gateway-litellm-fb-1  | Inside Max Parallel Request Pre-Call Hook
ai_gateway-litellm-fb-1  | ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None
ai_gateway-litellm-fb-1  | Final returned optional params: {'temperature': 0.2, 'stream': False, 'num_predict': 64}
ai_gateway-litellm-fb-1  | Async Wrapper: Completed Call, calling async_success_handler: <bound method Logging.async_success_handler of <litellm.litellm_core_utils.litellm_logging.Logging object at 0x702dff84b9d0>>
ai_gateway-litellm-fb-1  | Logging Details LiteLLM-Async Success Call, cache_hit=None
ai_gateway-litellm-fb-1  | Async success callbacks: Got a complete streaming response
ai_gateway-litellm-fb-1  | INSIDE parallel request limiter ASYNC SUCCESS LOGGING
ai_gateway-litellm-fb-1  | updated_value in success call: {'current_requests': 0, 'current_tpm': 228, 'current_rpm': 3}, precise_minute: 2025-09-11-23-59
ai_gateway-litellm-fb-1  | updated_value in success call: {'current_requests': 0, 'current_tpm': 228, 'current_rpm': 0}, precise_minute: 2025-09-11-23-59
ai_gateway-litellm-fb-1  | updated_value in success call: {'current_requests': 0, 'current_tpm': 228, 'current_rpm': 3}, precise_minute: 2025-09-11-23-59
ai_gateway-litellm-fb-1  | INFO:     172.22.0.1:46822 - "POST /v1/chat/completions HTTP/1.1" 200 OK
ai_gateway-litellm-fb-1  | [92m23:59:29 - LiteLLM:WARNING[0m: utils.py:528 - `litellm.set_verbose` is deprecated. Please set `os.environ['LITELLM_LOG'] = 'DEBUG'` for debug logs.
ai_gateway-litellm-fb-1  | Initialized litellm callbacks, Async Success Callbacks: [<bound method Router.deployment_callback_on_success of <litellm.router.Router object at 0x702e019101a0>>, <litellm.proxy.hooks.model_max_budget_limiter._PROXY_VirtualKeyModelMaxBudgetLimiter object at 0x702e029482f0>, <litellm.proxy.hooks.max_budget_limiter._PROXY_MaxBudgetLimiter object at 0x702e018f1d10>, <litellm.proxy.hooks.parallel_request_limiter._PROXY_MaxParallelRequestsHandler object at 0x702e018f2c10>, <litellm.proxy.hooks.cache_control_check._PROXY_CacheControlCheck object at 0x702e018f2ad0>, <litellm_enterprise.proxy.hooks.managed_files._PROXY_LiteLLMManagedFiles object at 0x702e01910ec0>, <litellm._service_logger.ServiceLogging object at 0x702e02bb8b00>]
ai_gateway-litellm-fb-1  | Inside Max Parallel Request Pre-Call Hook
ai_gateway-litellm-fb-1  | ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None
ai_gateway-litellm-fb-1  | Final returned optional params: {'temperature': 0.2, 'stream': False, 'num_predict': 64}
ai_gateway-litellm-fb-1  | Async Wrapper: Completed Call, calling async_success_handler: <bound method Logging.async_success_handler of <litellm.litellm_core_utils.litellm_logging.Logging object at 0x702dfe820050>>
ai_gateway-litellm-fb-1  | Logging Details LiteLLM-Async Success Call, cache_hit=None
ai_gateway-litellm-fb-1  | Async success callbacks: Got a complete streaming response
ai_gateway-litellm-fb-1  | INSIDE parallel request limiter ASYNC SUCCESS LOGGING
ai_gateway-litellm-fb-1  | updated_value in success call: {'current_requests': 0, 'current_tpm': 298, 'current_rpm': 4}, precise_minute: 2025-09-11-23-59
ai_gateway-litellm-fb-1  | updated_value in success call: {'current_requests': 0, 'current_tpm': 298, 'current_rpm': 0}, precise_minute: 2025-09-11-23-59
ai_gateway-litellm-fb-1  | updated_value in success call: {'current_requests': 0, 'current_tpm': 298, 'current_rpm': 4}, precise_minute: 2025-09-11-23-59
ai_gateway-litellm-fb-1  | INFO:     172.22.0.1:46834 - "POST /v1/chat/completions HTTP/1.1" 200 OK
ai_gateway-litellm-fb-1  | INFO:     172.22.0.1:33856 - "GET /v1/models HTTP/1.1" 200 OK
ai_gateway-litellm-fb-1  | [92m10:05:30 - LiteLLM:WARNING[0m: utils.py:528 - `litellm.set_verbose` is deprecated. Please set `os.environ['LITELLM_LOG'] = 'DEBUG'` for debug logs.
ai_gateway-litellm-fb-1  | Initialized litellm callbacks, Async Success Callbacks: [<bound method Router.deployment_callback_on_success of <litellm.router.Router object at 0x702e019101a0>>, <litellm.proxy.hooks.model_max_budget_limiter._PROXY_VirtualKeyModelMaxBudgetLimiter object at 0x702e029482f0>, <litellm.proxy.hooks.max_budget_limiter._PROXY_MaxBudgetLimiter object at 0x702e018f1d10>, <litellm.proxy.hooks.parallel_request_limiter._PROXY_MaxParallelRequestsHandler object at 0x702e018f2c10>, <litellm.proxy.hooks.cache_control_check._PROXY_CacheControlCheck object at 0x702e018f2ad0>, <litellm_enterprise.proxy.hooks.managed_files._PROXY_LiteLLMManagedFiles object at 0x702e01910ec0>, <litellm._service_logger.ServiceLogging object at 0x702e02bb8b00>]
ai_gateway-litellm-fb-1  | Inside Max Parallel Request Pre-Call Hook
ai_gateway-litellm-fb-1  | ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None
ai_gateway-litellm-fb-1  | Final returned optional params: {'temperature': 0, 'stream': False}
ai_gateway-litellm-fb-1  | Async Wrapper: Completed Call, calling async_success_handler: <bound method Logging.async_success_handler of <litellm.litellm_core_utils.litellm_logging.Logging object at 0x702e018f2850>>
ai_gateway-litellm-fb-1  | Logging Details LiteLLM-Async Success Call, cache_hit=None
ai_gateway-litellm-fb-1  | Async success callbacks: Got a complete streaming response
ai_gateway-litellm-fb-1  | INSIDE parallel request limiter ASYNC SUCCESS LOGGING
ai_gateway-litellm-fb-1  | updated_value in success call: {'current_requests': 0, 'current_tpm': 44, 'current_rpm': 1}, precise_minute: 2025-09-12-10-05
ai_gateway-litellm-fb-1  | updated_value in success call: {'current_requests': 0, 'current_tpm': 44, 'current_rpm': 0}, precise_minute: 2025-09-12-10-05
ai_gateway-litellm-fb-1  | updated_value in success call: {'current_requests': 0, 'current_tpm': 44, 'current_rpm': 1}, precise_minute: 2025-09-12-10-05
ai_gateway-litellm-fb-1  | INFO:     172.22.0.1:33868 - "POST /v1/chat/completions HTTP/1.1" 200 OK
ai_gateway-litellm-fb-1  | [92m10:05:34 - LiteLLM:WARNING[0m: utils.py:528 - `litellm.set_verbose` is deprecated. Please set `os.environ['LITELLM_LOG'] = 'DEBUG'` for debug logs.
ai_gateway-litellm-fb-1  | Initialized litellm callbacks, Async Success Callbacks: [<bound method Router.deployment_callback_on_success of <litellm.router.Router object at 0x702e019101a0>>, <litellm.proxy.hooks.model_max_budget_limiter._PROXY_VirtualKeyModelMaxBudgetLimiter object at 0x702e029482f0>, <litellm.proxy.hooks.max_budget_limiter._PROXY_MaxBudgetLimiter object at 0x702e018f1d10>, <litellm.proxy.hooks.parallel_request_limiter._PROXY_MaxParallelRequestsHandler object at 0x702e018f2c10>, <litellm.proxy.hooks.cache_control_check._PROXY_CacheControlCheck object at 0x702e018f2ad0>, <litellm_enterprise.proxy.hooks.managed_files._PROXY_LiteLLMManagedFiles object at 0x702e01910ec0>, <litellm._service_logger.ServiceLogging object at 0x702e02bb8b00>]
ai_gateway-litellm-fb-1  | Inside Max Parallel Request Pre-Call Hook
ai_gateway-litellm-fb-1  | ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None
ai_gateway-litellm-fb-1  | Final returned optional params: {'temperature': 0, 'stream': False}
ai_gateway-litellm-fb-1  | Async Wrapper: Completed Call, calling async_success_handler: <bound method Logging.async_success_handler of <litellm.litellm_core_utils.litellm_logging.Logging object at 0x702e018f1310>>
ai_gateway-litellm-fb-1  | Logging Details LiteLLM-Async Success Call, cache_hit=None
ai_gateway-litellm-fb-1  | Async success callbacks: Got a complete streaming response
ai_gateway-litellm-fb-1  | INSIDE parallel request limiter ASYNC SUCCESS LOGGING
ai_gateway-litellm-fb-1  | updated_value in success call: {'current_requests': 0, 'current_tpm': 90, 'current_rpm': 2}, precise_minute: 2025-09-12-10-05
ai_gateway-litellm-fb-1  | updated_value in success call: {'current_requests': 0, 'current_tpm': 46, 'current_rpm': 0}, precise_minute: 2025-09-12-10-05
ai_gateway-litellm-fb-1  | updated_value in success call: {'current_requests': 0, 'current_tpm': 90, 'current_rpm': 2}, precise_minute: 2025-09-12-10-05
ai_gateway-litellm-fb-1  | INFO:     172.22.0.1:33882 - "POST /v1/chat/completions HTTP/1.1" 200 OK
ai_gateway-litellm-fb-1  | [92m10:05:35 - LiteLLM:WARNING[0m: utils.py:528 - `litellm.set_verbose` is deprecated. Please set `os.environ['LITELLM_LOG'] = 'DEBUG'` for debug logs.
ai_gateway-litellm-fb-1  | Initialized litellm callbacks, Async Success Callbacks: [<bound method Router.deployment_callback_on_success of <litellm.router.Router object at 0x702e019101a0>>, <litellm.proxy.hooks.model_max_budget_limiter._PROXY_VirtualKeyModelMaxBudgetLimiter object at 0x702e029482f0>, <litellm.proxy.hooks.max_budget_limiter._PROXY_MaxBudgetLimiter object at 0x702e018f1d10>, <litellm.proxy.hooks.parallel_request_limiter._PROXY_MaxParallelRequestsHandler object at 0x702e018f2c10>, <litellm.proxy.hooks.cache_control_check._PROXY_CacheControlCheck object at 0x702e018f2ad0>, <litellm_enterprise.proxy.hooks.managed_files._PROXY_LiteLLMManagedFiles object at 0x702e01910ec0>, <litellm._service_logger.ServiceLogging object at 0x702e02bb8b00>]
ai_gateway-litellm-fb-1  | Inside Max Parallel Request Pre-Call Hook
ai_gateway-litellm-fb-1  | ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None
ai_gateway-litellm-fb-1  | Final returned optional params: {'temperature': 0, 'stream': False}
ai_gateway-litellm-fb-1  | Async Wrapper: Completed Call, calling async_success_handler: <bound method Logging.async_success_handler of <litellm.litellm_core_utils.litellm_logging.Logging object at 0x702e018f1bd0>>
ai_gateway-litellm-fb-1  | Logging Details LiteLLM-Async Success Call, cache_hit=None
ai_gateway-litellm-fb-1  | Async success callbacks: Got a complete streaming response
ai_gateway-litellm-fb-1  | INSIDE parallel request limiter ASYNC SUCCESS LOGGING
ai_gateway-litellm-fb-1  | updated_value in success call: {'current_requests': 0, 'current_tpm': 139, 'current_rpm': 3}, precise_minute: 2025-09-12-10-05
ai_gateway-litellm-fb-1  | updated_value in success call: {'current_requests': 0, 'current_tpm': 93, 'current_rpm': 0}, precise_minute: 2025-09-12-10-05
ai_gateway-litellm-fb-1  | updated_value in success call: {'current_requests': 0, 'current_tpm': 139, 'current_rpm': 3}, precise_minute: 2025-09-12-10-05
ai_gateway-litellm-fb-1  | INFO:     172.22.0.1:33884 - "POST /v1/chat/completions HTTP/1.1" 200 OK
ai_gateway-litellm-fb-1  | INFO:     172.22.0.1:39084 - "GET /v1/models HTTP/1.1" 200 OK
