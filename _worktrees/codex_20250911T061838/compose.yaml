name: ai_stack
services:
  ollama:
    image: ollama/ollama:latest
    restart: always
    ports: ["11434:11434"]
    volumes:
      - /data/ollama:/root/.ollama
    gpus: all
    healthcheck:
      test: ["CMD", "curl", "-fsS", "http://localhost:11434/api/tags"]
      interval: 10s
      timeout: 5s
      retries: 30

  qdrant:
    image: qdrant/qdrant:latest
    restart: always
    ports: ["6333:6333"]
    volumes:
      - /data/qdrant:/qdrant/storage
    healthcheck:
      test: ["CMD-SHELL", "wget -q -O - http://localhost:6333/readyz >/dev/null 2>&1 || exit 1"]
      interval: 10s
      timeout: 5s
      retries: 30

  litellm:
    image: litellm/litellm:latest
    platform: "linux/amd64"
    restart: always
    ports: ["4000:4000"]
    env_file:
      - /etc/ai_stack/env/openai.env
    environment:
      - LITELLM_CONFIG=/app/config/litellm.yaml
      - LITELLM_LOG=DEBUG
    volumes:
      - /data/stack/ai_gateway/litellm.yaml:/app/config/litellm.yaml:ro
      - /data/stack/ai_gateway/logs:/app/logs
      - /data/stack/ai_gateway/cache:/app/cache
    depends_on:
      ollama:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "curl", "-fsS", "http://localhost:4000/v1/models"]
      interval: 10s
      timeout: 5s
      retries: 30

  openwebui:
    image: ghcr.io/open-webui/open-webui:latest
    platform: "linux/amd64"
    restart: always
    ports: ["3000:8080"]
    environment:
      - OLLAMA_BASE_URL=http://ollama:11434
      - OPENAI_API_BASE=http://litellm:4000/v1
    volumes:
      - /data/openwebui:/app/backend/data
    depends_on:
      litellm:
        condition: service_healthy
