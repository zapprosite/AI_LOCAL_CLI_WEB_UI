services:
  ollama:
    image: ollama/ollama:latest
    restart: unless-stopped
    networks: [ai_stack_net]
    ports: ["11434:11434"]
    volumes:
      - /data/ollama:/root/.ollama

  litellm:
    image: ghcr.io/berriai/litellm:main-stable
    restart: unless-stopped
    networks: [ai_stack_net]
    depends_on: [ollama]
    ports: ["4000:4000"]
    env_file: [.env]
    environment:
      - LITELLM_MASTER_KEY=${LITELLM_MASTER_KEY}
    volumes:
      - ./config:/config:ro
    # The image entrypoint invokes litellm; provide only flags here
    command: >
      --config /config/litellm-config.yaml
      --host 0.0.0.0
      --port 4000

  openwebui:
    image: ghcr.io/open-webui/open-webui:latest
    restart: unless-stopped
    networks: [ai_stack_net]
    depends_on: [ollama]
    ports: ["3000:8080"]
    environment:
      - OLLAMA_BASE_URL=http://ollama:11434
    volumes:
      - /data/openwebui:/app/backend/data

  qdrant:
    image: qdrant/qdrant:latest
    restart: unless-stopped
    networks: [ai_stack_net]
    ports: ["6333:6333"]
    volumes:
      - /data/qdrant:/qdrant/storage

networks:
  ai_stack_net:
    external: true
