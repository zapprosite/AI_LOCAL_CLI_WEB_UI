litellm_settings:
  request_timeout: 65
  telemetry: false
  set_verbose: true
  drop_params: true
  num_retries: 2
  retry_policy: exponential

proxy_logging:
  # Se quiser persistir fora do container, mapeie via overlay de logs
  log_file: /tmp/litellm_fb.log

model_list:
  - model_name: code.router
    litellm_params: { model: ollama/qwen2.5-coder:14b, api_base: http://ollama:11434 }
  - model_name: docs.router
    litellm_params: { model: ollama/qwen2.5:7b-instruct, api_base: http://ollama:11434 }
  - model_name: search.router
    # Ajuste se usar outro tag disponível (ver PROMPT 24 do Codex)
    litellm_params: { model: ollama/llama3:8b, api_base: http://ollama:11434 }
  - model_name: openai.gpt5
    litellm_params: { model: openai/gpt-5 }

router_settings:
  # Expor aliases como "model" utilizável e visível em /v1/models
  model_group_alias:
    code.hybrid:   code.router
    docs.hybrid:   docs.router
    search.hybrid: search.router
    # atalhos para forçar remoto manualmente
    code.remote:   openai.gpt5
    docs.remote:   openai.gpt5
    search.remote: openai.gpt5

  # Se o primário falhar → tenta o remoto
  fallbacks:
    - code.hybrid:   ["openai.gpt5"]
    - docs.hybrid:   ["openai.gpt5"]
    - search.hybrid: ["openai.gpt5"]
    - code.router:   ["openai.gpt5"]
    - docs.router:   ["openai.gpt5"]
    - search.router: ["openai.gpt5"]

  # Opcional: limitar tentativas de fallback
  max_fallbacks: 1
